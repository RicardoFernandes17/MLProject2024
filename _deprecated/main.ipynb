{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: cartopy in /opt/homebrew/lib/python3.11/site-packages (0.24.1)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.2)\n",
            "Requirement already satisfied: chardet in /opt/homebrew/lib/python3.11/site-packages (5.2.0)\n",
            "Requirement already satisfied: xgboost in /opt/homebrew/lib/python3.11/site-packages (2.1.4)\n",
            "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/lib/python3.11/site-packages (from cartopy) (1.26.4)\n",
            "Requirement already satisfied: shapely>=1.8 in /opt/homebrew/lib/python3.11/site-packages (from cartopy) (2.0.7)\n",
            "Requirement already satisfied: packaging>=21 in /Users/ricardofernandes/Library/Python/3.11/lib/python/site-packages (from cartopy) (23.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /opt/homebrew/lib/python3.11/site-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /opt/homebrew/lib/python3.11/site-packages (from cartopy) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ricardofernandes/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from pyproj>=3.3.1->cartopy) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ricardofernandes/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install cartopy pandas matplotlib chardet xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeatures\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Local\n",
        "from jaguar_feature_scaling import get_added_features_plot,get_temporal_analysis_plot\n",
        "from helper import get_dataset_with_copy, calculate_group_directions, calculate_group_distances, remove_outliers, create_time_window_features,calculate_movement_features, classify_movement_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pandas configuration\n",
        "Configure pandas display settings to show more data in our notebook outputs\n",
        "This helps us see more rows and columns when examining our dataframes, rather than having them truncated with ellipsis (...) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will be set to see most of the infomation of any print that i make\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 1000); "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File reading and copy variable declaration\n",
        "We create two versions of our dataframe: main and copy\n",
        "The copy preserves our original, untouched data as a backup reference, while the main dataframe will be used for active analysis and transformations.\n",
        "This is particularly useful in Jupyter notebooks where we can always refer back to the original state of our data without reloading the file or having to restart the notebook to run it all again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/raw/jaguar_movement_data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Detect the file encoding\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m jaguar_data_original, jaguar_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_with_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/raw/jaguar_movement_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m jaguar_info_original, jaguar_info \u001b[38;5;241m=\u001b[39m get_dataset_with_copy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/jaguar_additional_information.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Code/ML/_deprecated/helper.py:6\u001b[0m, in \u001b[0;36mget_dataset_with_copy\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset_with_copy\u001b[39m(file_path):\n\u001b[0;32m----> 6\u001b[0m     dataset_original \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset_original\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset_original, dataset\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/jaguar_movement_data.csv'"
          ]
        }
      ],
      "source": [
        "# Detect the file encoding\n",
        "jaguar_data_original, jaguar_data = get_dataset_with_copy('data/raw/jaguar_movement_data.csv')\n",
        "\n",
        "jaguar_info_original, jaguar_info = get_dataset_with_copy('data/raw/jaguar_additional_information.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial Data Exploration\n",
        "Display basic information about both datasets including their structure and dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_data_original.head())\n",
        "print(jaguar_data_original.shape)\n",
        "print(jaguar_info_original.head())\n",
        "print(jaguar_info_original.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check detailed information about data types and null values in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_data_original.info())\n",
        "print(jaguar_info_original.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gender Analysis\n",
        "Analyze and visualize gender distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_info_original['Sex'].unique())\n",
        "gender = jaguar_info_original['Sex'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create bar plot for gender distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gender.plot(kind='bar',figsize=(10,10))\n",
        "plt.title('Number of birds detected')\n",
        "plt.xlabel('Sex')\n",
        "plt.ylabel('Number')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Quality Check\n",
        "Check for missing values in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_data_original.isnull().sum())\n",
        "print(\"---------\")\n",
        "print(jaguar_info_original.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check unique values in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_data_original.nunique())\n",
        "print(\"---------\")\n",
        "print(jaguar_info_original.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Missing Value Treatment\n",
        "Since there are null values in the columns \"Estimated Age\" and \"Weight\" (jaguar_info) we will be filling them 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_info[\"Estimated Age\"]= jaguar_info[\"Estimated Age\"].fillna(value=0)\n",
        "jaguar_info[\"Weight\"]= jaguar_info[\"Weight\"].fillna(value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display all columns in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'jaguar_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjaguar_data\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(jaguar_info\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jaguar_data' is not defined"
          ]
        }
      ],
      "source": [
        "print(jaguar_data.columns.to_list())\n",
        "print(jaguar_info.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename columns and drop unnecessary ones.\n",
        "We remove unnecesary or unique value columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_data.rename(columns={'location.long': 'longitude', 'location.lat': 'latitude', 'individual.local.identifier (ID)': 'individual_id' }, inplace=True)\n",
        "jaguar_data.drop(['Event_ID', 'individual.taxon.canonical.name','tag.local.identifier', 'study.name', 'country'], axis=1,inplace=True)\n",
        "jaguar_info.rename(columns={'ID':'individual_id','Sex': 'sex', 'Estimated Age': 'age', 'Weight': 'weight' }, inplace=True)\n",
        "jaguar_info.drop(['Collar Type', 'Collar Brand','Planned Schedule', 'Project Leader', 'Contact'], axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert timestamp to datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_data['timestamp'] = pd.to_datetime(jaguar_data['timestamp'], errors='coerce')\n",
        "print(jaguar_data.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display all the columns in both datasets to confirm the changes we've done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_data.columns.to_list())\n",
        "print(jaguar_info.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Merge and Grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge datasets.\n",
        "We merge them using the id of the individual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew = jaguar_data.merge(jaguar_info, on='individual_id', how='left')\n",
        "print(jaguar_datanew)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grouping. We create a dictionary where each key is the id of the jaguar and the value is their tracking and information data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_groups = {individual_id: group for individual_id, group in jaguar_datanew.groupby('individual_id')}\n",
        "print(jaguar_groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print record count for each jaguar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for jaguar_id, subset in jaguar_groups.items():\n",
        "    print(f\"Jaguar {jaguar_id}: {len(subset)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Time Features\n",
        "Adding time-based features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew['hour'] = jaguar_datanew['timestamp'].dt.hour\n",
        "jaguar_datanew['day'] = jaguar_datanew['timestamp'].dt.day\n",
        "jaguar_datanew['month'] = jaguar_datanew['timestamp'].dt.month\n",
        "jaguar_datanew['year'] = jaguar_datanew['timestamp'].dt.year\n",
        "jaguar_datanew['dayofweek'] = jaguar_datanew['timestamp'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create time period categories:\n",
        "- 0 (24) to 6 : Night\n",
        "- 6 to 12: Morning\n",
        "- 12 to 18: Afternoon\n",
        "- 18 to 24 (0): Evening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew['time_of_day'] = pd.cut(jaguar_datanew['hour'], bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Movement Analysis Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be calculating:\n",
        "- Time differences between consecutive points for each jaguar\n",
        "- Distances\n",
        "- Speed\n",
        "- Direction of movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew['time_diff'] = jaguar_datanew.groupby('individual_id')['timestamp'].diff()\n",
        "jaguar_datanew['distance'] = jaguar_datanew.groupby('individual_id', group_keys=False).apply(calculate_group_distances)\n",
        "jaguar_datanew['time_diff_hours'] = jaguar_datanew['time_diff'].dt.total_seconds() / 3600\n",
        "jaguar_datanew['speed'] = jaguar_datanew['distance'] / jaguar_datanew['time_diff_hours'].replace({0: np.nan})\n",
        "jaguar_datanew['direction'] = jaguar_datanew.groupby('individual_id', group_keys=False).apply(calculate_group_directions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Cleaning and Validation\n",
        "Handle infinite values and outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew = jaguar_datanew.replace([np.inf, -np.inf], np.nan)\n",
        "jaguar_datanew = remove_outliers(jaguar_datanew, 'speed')\n",
        "jaguar_datanew = remove_outliers(jaguar_datanew, 'distance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fill missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_datanew['speed'] = jaguar_datanew['speed'].fillna(method='ffill')\n",
        "jaguar_datanew['distance'] = jaguar_datanew['distance'].fillna(method='ffill')\n",
        "jaguar_datanew['direction'] = jaguar_datanew['direction'].fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results Validation\n",
        "Display processed data sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_datanew[['individual_id', 'timestamp', 'latitude', 'longitude', 'distance', 'speed', 'direction']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(jaguar_datanew['distance'].describe())\n",
        "print(\"-----\")\n",
        "print(jaguar_datanew['speed'].describe())\n",
        "print(\"-----\")\n",
        "print(jaguar_datanew['direction'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ploting statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "features_plt = get_added_features_plot(jaguar_datanew)\n",
        "features_plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Additional temporal analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional temporal analysis\n",
        "travaled_distance_plt = get_temporal_analysis_plot(jaguar_datanew)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Individual jaguar movement patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(40, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "# Set the extent to the whole world (-180 to 180 longitude, -90 to 90 latitude)\n",
        "# ax.set_global()\n",
        "\n",
        "# Add map features\n",
        "ax.add_feature(cfeatures.LAND, edgecolor='black')\n",
        "ax.add_feature(cfeatures.OCEAN)\n",
        "ax.add_feature(cfeatures.COASTLINE)\n",
        "ax.add_feature(cfeatures.BORDERS, linestyle=':')\n",
        "\n",
        "# Plot each jaguar's movement\n",
        "\n",
        "for jaguar_id in jaguar_datanew['individual_id'].unique():\n",
        "    jaguar_subset = jaguar_datanew[jaguar_datanew['individual_id'] == jaguar_id]\n",
        "    ax.plot(jaguar_subset['longitude'], jaguar_subset['latitude'], \n",
        "             label=f'Jaguar {jaguar_id}', alpha=1)\n",
        "\n",
        "# Labels and legend\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "ax.set_title(\"Jaguar Movement Paths on World Map\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMovement Statistics by Time of Day:\")\n",
        "print(jaguar_datanew.groupby('time_of_day')[['speed', 'distance']].agg(['mean', 'std']).round(3))\n",
        "\n",
        "print(\"\\nMovement Statistics by Individual:\")\n",
        "print(jaguar_datanew.groupby('individual_id')[['speed', 'distance']].agg(['mean', 'std']).round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlations\n",
        "As we can see in the plotted correlation matrix underneath:\n",
        "There is a strong correlation between the following columns:\n",
        "    - Latitude and longitude (Negative)\n",
        "There is also moderate correlation between:\n",
        "    - Timestamp and latitude (positive)\n",
        "There is a weak or no correlation at all between (values closer to 0 both negative and positive):\n",
        "    - Individual id and latitude\n",
        "    - Individual id and longitude\n",
        "    - Timestamp and longitude\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_columns = jaguar_datanew.select_dtypes(include=[np.number]).columns\n",
        "correlation_data = jaguar_datanew[numeric_columns].corr()\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(correlation_data,  annot=True)\n",
        "\n",
        "#Filtering Out Self-Correlations\n",
        "# Unstack the correlation matrix\n",
        "correlation_pairs = correlation_data.unstack()\n",
        "\n",
        "# Filter out self-correlations (where feature pairs are the same)\n",
        "filtered_correlation_pairs = correlation_pairs[correlation_pairs.index.get_level_values(0) != correlation_pairs.index.get_level_values(1)]\n",
        "\n",
        "# Sort the remaining pairs in descending order of correlation\n",
        "filtered_correlation_pairs = filtered_correlation_pairs.sort_values(kind=\"quicksort\", ascending=True)\n",
        "\n",
        "print(filtered_correlation_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# for jaguar_id, subset in jaguar_groups.items():\n",
        "#     ax1 = subset.head().plot.scatter(x='timestamp',\n",
        "#                        y='longitude',\n",
        "#                        c='DarkBlue')\n",
        "    #print(subset.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Splitting dataset (unsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jaguar_data_copy = jaguar_data.copy()\n",
        "X_copy = jaguar_data_copy.drop('individual_id', axis=1)\n",
        "X_copy.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_copy = jaguar_data_copy['individual_id']         # we want to predict y using X\n",
        "y_copy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#1 - Split the dataset: tes=25%; training=75%\n",
        "# test size=25%\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_copy,y_copy,test_size=0.25,random_state=40)\n",
        "\n",
        "print(len(X_train)*100/len(jaguar_data_copy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_features = []\n",
        "for jaguar_id in jaguar_datanew['individual_id'].unique():\n",
        "    # Get data for current jaguar\n",
        "    jaguar_data = jaguar_datanew[jaguar_datanew['individual_id'] == jaguar_id].copy()\n",
        "    \n",
        "    # Calculate window features without setting index beforehand\n",
        "    window_stats = create_time_window_features(jaguar_data)\n",
        "    window_stats['individual_id'] = jaguar_id\n",
        "    window_features.append(window_stats)\n",
        "    \n",
        "window_features_df = pd.concat(window_features).reset_index()\n",
        "\n",
        "window_features_df = calculate_movement_features(window_features_df)\n",
        "\n",
        "# 3. Add temporal context features\n",
        "window_features_df['hour'] = window_features_df['timestamp'].dt.hour\n",
        "window_features_df['is_night'] = (window_features_df['hour'] >= 18) | (window_features_df['hour'] <= 6)\n",
        "window_features_df['is_peak_activity'] = window_features_df['hour'].isin([5,6,7,17,18,19])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_features_df['movement_state'] = window_features_df.apply(classify_movement_state, axis=1)\n",
        "\n",
        "# Let's visualize our new features\n",
        "plt.figure(figsize=(15, 30))\n",
        "\n",
        "# Plot 1: Movement States Distribution\n",
        "plt.subplot(4, 1, 1)\n",
        "sns.countplot(data=window_features_df, x='movement_state')\n",
        "plt.title('Distribution of Movement States')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 2: Speed vs Area Covered\n",
        "plt.subplot(4, 1, 2)\n",
        "sns.scatterplot(data=window_features_df, x='speed_mean', y='area_covered', \n",
        "                hue='movement_state', alpha=0.6)\n",
        "plt.title('Speed vs Area Covered by Movement State')\n",
        "\n",
        "# Plot 3: Movement Patterns by Time of Day\n",
        "plt.subplot(4, 1, 3)\n",
        "sns.boxplot(data=window_features_df, x='hour', y='movement_intensity')\n",
        "plt.title('Movement Intensity by Hour')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 4: Path Efficiency Distribution\n",
        "plt.subplot(4, 1, 4)\n",
        "sns.boxplot(data=window_features_df, x='movement_state', y='path_efficiency')\n",
        "plt.title('Path Efficiency by Movement State')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary statistics of our engineered features\n",
        "print(\"\\nSummary Statistics of Engineered Features:\")\n",
        "print(window_features_df.describe().round(3))\n",
        "\n",
        "print(\"\\nMovement State Distribution:\")\n",
        "print(window_features_df['movement_state'].value_counts(normalize=True).round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's prepare our features and target\n",
        "# Drop non-feature columns and handle any remaining NaN values\n",
        "feature_columns = ['speed_mean', 'speed_max', 'speed_std', 'distance_sum', 'distance_mean', 'direction_mean', 'direction_std', 'area_covered', 'movement_intensity', 'path_efficiency', 'direction_variability', 'hour', 'is_night', 'is_peak_activity']\n",
        "\n",
        "X = window_features_df[feature_columns].fillna(0)\n",
        "y = window_features_df['movement_state']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dictionary of pipelines with different models\n",
        "# pipelines = {\n",
        "#     'random_forest': Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "#     ]),\n",
        "    \n",
        "#     'svm': Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('classifier', SVC(kernel='rbf', probability=True))\n",
        "#     ]),\n",
        "    \n",
        "#     'logistic': Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('classifier', LogisticRegression(max_iter=1000))\n",
        "#     ]),\n",
        "    \n",
        "#     'xgboost': Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('classifier', xgb.XGBClassifier(random_state=42))\n",
        "#     ])\n",
        "# }\n",
        "\n",
        "pipelines = {\n",
        "    'random_forest': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=50, random_state=42)) \n",
        "    ]),\n",
        "    'logistic': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', LogisticRegression(max_iter=500))\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, pipeline in pipelines.items():\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'pipeline': pipeline,\n",
        "        'train_score': pipeline.score(X_train, y_train),\n",
        "        'test_score': pipeline.score(X_test, y_test),\n",
        "        'cv_scores': cross_val_score(pipeline, X_train, y_train, cv=5),\n",
        "        'predictions': y_pred,\n",
        "        'classification_report': classification_report(y_test, y_pred)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Print results\n",
        "for name, result in results.items():\n",
        "    print(f\"\\n{name.upper()} RESULTS:\")\n",
        "    print(f\"Training Score: {result['train_score']:.4f}\")\n",
        "    print(f\"Test Score: {result['test_score']:.4f}\")\n",
        "    print(f\"Cross-validation Scores: {result['cv_scores'].mean():.4f} (+/- {result['cv_scores'].std() * 2:.4f})\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(result['classification_report'])\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Model Comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "model_scores = {name: result['test_score'] for name, result in results.items()}\n",
        "plt.bar(model_scores.keys(), model_scores.values())\n",
        "plt.title('Model Test Scores Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Plot 2: Cross-validation Scores\n",
        "plt.subplot(2, 2, 2)\n",
        "cv_means = [result['cv_scores'].mean() for result in results.values()]\n",
        "cv_stds = [result['cv_scores'].std() for result in results.values()]\n",
        "plt.errorbar(results.keys(), cv_means, yerr=cv_stds, fmt='o')\n",
        "plt.title('Cross-validation Scores with Standard Deviation')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('CV Score')\n",
        "\n",
        "# Plot 3: Feature Importance (for Random Forest)\n",
        "rf_pipeline = results['random_forest']['pipeline']\n",
        "rf_classifier = rf_pipeline.named_steps['classifier']\n",
        "importances = pd.Series(\n",
        "    rf_classifier.feature_importances_,\n",
        "    index=feature_columns\n",
        ")\n",
        "plt.subplot(2, 2, 3)\n",
        "importances.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "\n",
        "# Plot 4: Confusion Matrix for best model\n",
        "best_model_name = max(results.items(), key=lambda x: x[1]['test_score'])[0]\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.heatmap(\n",
        "    confusion_matrix(y_test, best_predictions, normalize='true'),\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    xticklabels=rf_classifier.classes_,\n",
        "    yticklabels=rf_classifier.classes_\n",
        ")\n",
        "plt.title(f'Confusion Matrix ({best_model_name})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the best model\n",
        "best_model = results[best_model_name]['pipeline']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
